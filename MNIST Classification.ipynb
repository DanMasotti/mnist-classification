{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(19)\n",
    "\n",
    "# loading the MNIST data using this neat library\n",
    "from mnist import MNIST\n",
    "data = MNIST('./data')\n",
    "X_train, Y_train = data.load_training()\n",
    "X_test, Y_test = data.load_testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and encoding the labels using one-hot encoding.  This is so that we make our output from the NN a 10-d vector whose components correspond to a vector whose ith component is the label i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizing\n",
    "X_train = X_train/255\n",
    "X_test = X_test/ 255\n",
    "\n",
    "print(Y_test[0])\n",
    "# one-hot-encoding y so that we can compare probability distributions instead of categories\n",
    "num_classes = 10\n",
    "targets = np.array(np.array(Y_train)).reshape(-1)\n",
    "Y_train = np.eye(num_classes)[targets]\n",
    "\n",
    "targets = np.array(np.array(Y_test)).reshape(-1)\n",
    "Y_test = np.eye(num_classes)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural-Network in NumPy to solve the MNIST Classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, writing the non-linear activation function, 'sigmoid'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g(x) = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x: 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the output, we want a probability distribution on labels.  These are called logits.  The way we get this is by 'soft-maxing' the output of the final layer.  This is the generalization of sigmoid activation for multi-class problems.  We subtract the max from the numerator and denominator in the code for numerical stability in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^Ne^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = lambda x: np.exp(x-np.max(x,axis=1,keepdims=True))/ \\\n",
    "                    (np.sum(np.exp(x-np.max(x,axis=1,keepdims=True)),axis=1,keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have what we need to create the foward pass algorithm (the neural network), which defines the function approximation, in this case a pmf on labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{p}(x) = \\text{softmax} \\left[ g(g\\left[ g(x)w_1 + b_1\\right] w_2 + b_2)w_3 + b_3 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose a two hidden layer neural network, because this is the architecture we talked about in class when we wanted to approximate bump functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_pass(X, W1,W2,W3, b1,b2,b3):\n",
    "    # in: feature vector X, current weights and biases\n",
    "    # out: the output from each layer (two hidden layers and an output layer)\n",
    "    \n",
    "    # integrate and fire\n",
    "    L1 = g(np.dot(X, W1) + b1)\n",
    "    \n",
    "    L2 = g(np.dot(L1, W2) + b2)\n",
    "    \n",
    "    # the output pmf\n",
    "    L3 = softmax(np.dot(L2, W3) + b3)\n",
    "    \n",
    "    return L1,L2,L3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, defining a cost function (we use this to update the weights in the backpropagation algorithm).  We like to choose a cost function that makes sense with our output units, which live in the space (0,1).  Let's use binary cross entropy which measures the mismatch between our predicted distribution on labels and the true distribution.  This is the standard cost function for this problem and can be motivated by an information theory perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(w) = H(p,\\hat{p})  = \\frac{-1}{\\text{# samples}}\\sum_{i} p_i \\log \\hat{p}_i(w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fcn(Y_pred, Y_true):\n",
    "    # for numerical stability\n",
    "    eps = np.finfo(float).eps\n",
    "    Y_pred = np.clip(Y_pred, eps, 1. - eps)\n",
    "    cost = - np.mean(np.log(Y_pred) * Y_true) \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some additional gradient functions for the updating procedure.  When we actually use g', the input is already the output g from the last layer, so we don't reapply sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_prime = lambda gx: gx*(1-gx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you find the update rule (the derivative of the cost with respect to the weights), a lot of things cancel when using binary cross entropy and softmax together.  The following is the gradient for the last output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_prime = lambda Y_true, Y_pred: (Y_pred-Y_true)/Y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to define the backpropagation algorithm, which is the way we optimize these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note the network:\n",
    "$$\n",
    "l_1 = g(xw_1 + b_1)\\\\\n",
    "l_2 = g(l_1w_2 + b_2)\\\\\n",
    "l_3 = \\text{softmax}(l_2w_3 + b_3)\\\\\n",
    "$$\n",
    "So recursively taking the gradients:\n",
    "$$\n",
    "\\Delta l_3 = \\frac{\\partial C}{\\partial w_3} = l_3 - y _{\\text{true}} \\\\\n",
    "\\Delta l_2 = \\frac{\\partial l_3}{\\partial w_2} = \\Delta l_3 w_3 g'(l_2) \\\\\n",
    "\\Delta l_1 = \\frac{\\partial l_2}{\\partial w_1} = \\Delta l_2 w_2 g'(l_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, Y, W1,W2,W3, b1,b2,b3, L1,L2,L3):\n",
    "    # Input: outputs from forward pass\n",
    "    # Output: the updated weights, biases from gradient descent\n",
    "\n",
    "    learning_rate = .5\n",
    "    # computing the partials for the last layer, which is used for the next, and so on\n",
    "    L3_delta = cost_prime(L3, Y)\n",
    "    \n",
    "    #played around with the dots and the transposes until I got the desired dimensions\n",
    "    W3 = W3 + learning_rate * np.dot(L2.T, L3_delta)\n",
    "    b3 = b3 + learning_rate * np.sum(L3_delta, axis = 0, keepdims = True)\n",
    "    \n",
    "    # next down\n",
    "    L2_delta = np.dot(L3_delta, W3.T) * g_prime(L2) \n",
    "    W2 = W2 + learning_rate * np.dot(L1.T, L2_delta)\n",
    "    b2 = b2 + learning_rate * np.sum(L2_delta, axis = 0)\n",
    "    \n",
    "    #first layer\n",
    "    L1_delta = np.dot(L2_delta, W2.T) * g_prime(L1) \n",
    "    W1 = W1 + learning_rate * np.dot(X.T, L1_delta)\n",
    "    b1 = b1 + learning_rate * np.sum(L1_delta, axis = 0)\n",
    "    \n",
    "    return W1,W2,W3, b1,b2,b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose a random initialization and a hidden size of 128 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_sz = 128\n",
    "\n",
    "in_sz = X_train.shape[1]\n",
    "out_sz = Y_train.shape[1]\n",
    "\n",
    "W1 = np.random.randn(in_sz, dim_sz)\n",
    "b1 = np.zeros((1, dim_sz))\n",
    "\n",
    "W2 = np.random.randn(dim_sz, dim_sz)\n",
    "b2 = np.zeros((1, dim_sz))\n",
    "\n",
    "W3 = np.random.randn(dim_sz, out_sz)\n",
    "b3 = np.zeros((1, out_sz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the weights through gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 | cost = 0.8429628346288437\n",
      "epoch = 100 | cost = 0.08870297715786424\n",
      "epoch = 200 | cost = 0.06633111300105028\n",
      "epoch = 300 | cost = 0.05643794296255471\n",
      "epoch = 400 | cost = 0.05043641834047603\n",
      "epoch = 500 | cost = 0.04623390500561551\n",
      "epoch = 600 | cost = 0.04304648717000773\n",
      "epoch = 700 | cost = 0.040501588263851765\n",
      "epoch = 800 | cost = 0.03839871869674471\n",
      "epoch = 900 | cost = 0.03661765489057807\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "training = np.zeros(epochs)\n",
    "for epoch in range(epochs):\n",
    "    L1,L2,L3 = foward_pass(X_train, W1,W2,W3, b1,b2,b3)\n",
    "    curr_cost = cost_fcn(L3,Y_train)\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch = {} | cost = {}'.format(epoch, curr_cost))\n",
    "    training[epoch] = curr_cost\n",
    "    W1,W2,W3, b1,b2,b3 = backward_pass(X_train, Y_train, W1,W2,W3, b1,b2,b3, L1,L2,L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cross entropy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on testing data:  88.75\n"
     ]
    }
   ],
   "source": [
    "L1,L2,L3 = foward_pass(X_test, W1,W2,W3, b1,b2,b3)\n",
    "\n",
    "acc = 0\n",
    "for x,y in zip(X_test, Y_test):\n",
    "    L1,L2,L3 = foward_pass(x, W1,W2,W3, b1,b2,b3)\n",
    "    s = L3.argmax()\n",
    "    # if s is the right label, increase accuracy\n",
    "    if s == np.argmax(y):\n",
    "        acc = acc + 1\n",
    "accuracy = acc/len(X_test)*100\n",
    "print(\"accuracy on testing data: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Net in TensorFlow for the MNIST Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "#visualization library\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "tf.random.set_seed(194)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizing\n",
    "X_train = X_train/255\n",
    "X_test = X_test/ 255\n",
    "\n",
    "# LeNet-5 accepts 32 by 32 image, so to make it work with the given 28 by 28 image, I pad with zeros\n",
    "X_train = np.pad(X_train,((0,0),(2,2),(2,2),(0,0)),'constant')\n",
    "X_test = np.pad(X_test,((0,0),(2,2),(2,2),(0,0)),'constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use LeCunn's LeNet-5.  It accepts a 32 by 32 pixel image and returns a 10-d vector of values in (0,1) which corresponds to labels.  It consists of two convolutional layers, each of which uses average pooling to downsample and cut the number of parameters down, and two fully connected hidden layers. Tensorflow is Object Oreintated, so I instantiate a model object instead of writing things procedurally as I did before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LeNet_Original_Image.jpg\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeCun et al., 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 6)         60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 15, 15, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               69240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 81,194\n",
      "Trainable params: 81,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this model keeps track of all the weights and biases under the hood\n",
    "model = models.Sequential()\n",
    "\n",
    "# conv1\n",
    "model.add(layers.Conv2D(filters = 6, kernel_size=(3, 3), activation='relu', input_shape= (32,32,1)))\n",
    "# downsample the image\n",
    "model.add(layers.AveragePooling2D())\n",
    "\n",
    "#conv2 \n",
    "model.add(layers.Conv2D(filters = 16, kernel_size=(3, 3), activation='relu'))\n",
    "# downsample\n",
    "model.add(layers.AveragePooling2D())\n",
    "\n",
    "# just flatten the last tensor into vector\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# fully connected layers\n",
    "model.add(layers.Dense(units = 120, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units = 84, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "# visualize schematically\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the underlying computation graph and optimizing weights via backpropagation with Adam, which is a popular type of Stochastic Gradient Descent algorithm.  (It's Gradient Descent, but updates in batches of samples instead of using all at once, so it's stochastically marching down the loss function.  It also uses other parameters such as momentum).  We're trying to minimize cross entropy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tensorboard to visualize the loss and accuracy over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 22s 366us/sample - loss: 0.2111 - accuracy: 0.9357\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 21s 345us/sample - loss: 0.0685 - accuracy: 0.9792\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 20s 338us/sample - loss: 0.0491 - accuracy: 0.9847\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 20s 338us/sample - loss: 0.0393 - accuracy: 0.9877\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 21s 356us/sample - loss: 0.0312 - accuracy: 0.9901\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 20s 333us/sample - loss: 0.0249 - accuracy: 0.9922\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 19s 316us/sample - loss: 0.0224 - accuracy: 0.9930\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 19s 319us/sample - loss: 0.0183 - accuracy: 0.9941\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 19s 313us/sample - loss: 0.0160 - accuracy: 0.9948\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 20s 327us/sample - loss: 0.0128 - accuracy: 0.9957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13909f9e8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = X_train,y = Y_train, epochs=10, callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 2963), started 0:53:04 ago. (Use '!kill 2963' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-86b6164082f76148\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-86b6164082f76148\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 177us/sample - loss: 0.0421 - accuracy: 0.9892\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
